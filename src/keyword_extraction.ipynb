{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python389jvsc74a57bd006384a114dfeabc4d7df56da5949e2c3dc942693f851f9c7215e7be631f8b4a1",
   "display_name": "Python 3.8.9 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "06384a114dfeabc4d7df56da5949e2c3dc942693f851f9c7215e7be631f8b4a1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metadata():\n",
    "    with open('./data/arxiv-metadata-oai-snapshot.json') as f:\n",
    "        for line in f:\n",
    "            yield line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def extract_abstarct_to_corpus(metadata):\n",
    "    abstracts = []\n",
    "    for paper in metadata:\n",
    "        paper = json.loads(paper)\n",
    "        abstract = paper['abstract']\n",
    "        if len(abstract) < 500:\n",
    "            abstracts.append(abstract)\n",
    "\n",
    "    with open('./data/corpus.txt', 'w', encoding='utf-8') as output:\n",
    "        for abstract in abstracts:\n",
    "            output.write(\"%s\\n\" % abstract)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "metadata = get_metadata()\n",
    "extract_abstarct_to_corpus(metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\georg\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from gensim import corpora, models\n",
    "import functools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stopword_list():\n",
    "    # 停用词表存储路径，每一行为一个词，按行读取进行加载\n",
    "    # 进行编码转换确保匹配准确率\n",
    "    stop_word_path = './data/stopwords.txt'\n",
    "    stopword_list = [sw.replace('\\n', '') for sw in open(stop_word_path).readlines()]\n",
    "    return stopword_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seg_to_list(sentence, pos=False):\n",
    "    ''' 分词方法 '''\n",
    "    return sentence.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def word_filter(seg_list, pos=False):\n",
    "    stopword_list = get_stopword_list() # 获取停用词表\n",
    "    filter_list = [] # 保存过滤后的结果\n",
    "\n",
    "    for seg in seg_list:\n",
    "        # 过滤停用词表中的词，以及长度为<2的词\n",
    "        if not seg in stopword_list and len(seg) > 1:\n",
    "            filter_list.append(re.sub(r'[^a-zA-Z0-9]', '', seg) )\n",
    "            \n",
    "    return filter_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(pos=False, corpus_path = './data/corpus.txt'):\n",
    "    '''\n",
    "        目的：\n",
    "            调用上面方法对数据集进行处理，处理后的每条数据仅保留非干扰词\n",
    "        参数：\n",
    "            1. 数据加载\n",
    "            2. pos: 是否词性标注的参数\n",
    "            3. corpus_path: 数据集路径\n",
    "    '''\n",
    "    doc_list = [] # 结果\n",
    "    for line in open(corpus_path, 'r'):\n",
    "        content = line.strip() # 每行的数据\n",
    "        seg_list = seg_to_list(content, pos) # 分词\n",
    "        filter_list = word_filter(seg_list, pos) # 过滤停用词\n",
    "        doc_list.append(filter_list) # 将处理后的结果保存到doc_list\n",
    "    return doc_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF的训练主要是根据数据集生成对应的IDF值字典，后续计算每个词的TF-IDF时，直接从字典中读取。\n",
    "\n",
    "def train_idf(doc_list):\n",
    "    idf_dic = {} # idf对应的字典\n",
    "    tt_count = len(doc_list) # 总文档数\n",
    "    # 每个词出现的文档数\n",
    "    for doc in doc_list: \n",
    "        for word in set(doc):\n",
    "            idf_dic[word] = idf_dic.get(word, 0.0) + 1.0\n",
    "    # 按公式转换为idf值，分母加1进行平滑处理\n",
    "    for k, v in idf_dic.items():\n",
    "        idf_dic[k] = math.log(tt_count/(1.0 + v))\n",
    "    # 对于没有在字典中的词，默认其尽在一个文档出现，得到默认idf值\n",
    "    default_idf = math.log(tt_count/(1.0))\n",
    "    return idf_dic, default_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA的训练时根据现有的数据集生成文档-主题分布矩阵和主题-词分布矩阵，Gensim中有实现好的方法，可以直接调用。\n",
    "\n",
    "def train_lda(self):\n",
    "    lda = models.LdaModel(self.corpus_tfidf, id2word=self.dictionary, num_topics=self.num_topics)\n",
    "    return lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 为了输出top关键词时，先按照关键词的计算分值排序，在得分相同时，根据关键词进行排序\n",
    "\n",
    "def cmp(e1, e2):\n",
    "    ''' 排序函数，用于topK关键词的按值排序 '''\n",
    "    import numpy as np\n",
    "    res = np.sign(e1[1] - e2[1])\n",
    "    if res != 0:\n",
    "        return res\n",
    "    else:\n",
    "        a = e1[0] + e2[0]\n",
    "        b = e2[0] + e1[0]\n",
    "        if a > b:\n",
    "            return 1\n",
    "        elif a == b:\n",
    "            return 0\n",
    "        else:\n",
    "            return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TfIdf(object):\n",
    "    # 四个参数分别是：训练好的idf字典，默认idf字典，处理后的待提取文本， 关键词数量\n",
    "    def __init__(self, idf_dic, default_idf, word_list, keyword_num):\n",
    "        self.idf_dic, self.default_idf = idf_dic, default_idf\n",
    "        self.word_list = word_list\n",
    "        self.tf_dic = self.get_tf_dic() # 统计tf值\n",
    "        self.keyword_num = keyword_num\n",
    "    \n",
    "    def get_tf_dic(self):\n",
    "        # 统计tf值\n",
    "        tf_dic = {}\n",
    "        for word in self.word_list:\n",
    "            tf_dic[word] = tf_dic.get(word, 0.0) + 1.0\n",
    "        tt_count = len(self.word_list)\n",
    "        for k, v in tf_dic.items():\n",
    "            tf_dic[k] = float(v) / tt_count # 根据tf求值公式\n",
    "        \n",
    "        return tf_dic\n",
    "\n",
    "    def get_tfidf(self):\n",
    "        # 计算tf-idf值\n",
    "        tfidf_dic = {}\n",
    "        for word in self.word_list:\n",
    "            idf = self.idf_dic.get(word, self.default_idf)\n",
    "            tf  = self.tf_dic.get(word, 0)\n",
    "            \n",
    "            tfidf = tf * idf\n",
    "            tfidf_dic[word] = tfidf\n",
    "        \n",
    "        tfidf_dic.items()\n",
    "        # 根据tf-idf排序，去排名前keyword_num的词作为关键词\n",
    "        for k, v in sorted(tfidf_dic.items(), key=functools.cmp_to_key(cmp), reverse=True)[:self.keyword_num]:\n",
    "            print(k + '/', end='')\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TopicModel(object):\n",
    "    # 三个传入参数：处理后的数据集，关键词数量，具体模型（LSI，LDA），主题数量\n",
    "    def __init__(self, doc_list, keyword_num, model='LSI', num_topics=4):\n",
    "        # 使用gensim接口，将文本转为向量化表示\n",
    "        # 先构建词空间\n",
    "        self.dictionary = corpora.Dictionary(doc_list)\n",
    "        # 使用BOW模型向量化\n",
    "        corpus = [self.dictionary.doc2bow(doc) for doc in doc_list]\n",
    "        # 对每个词，根据tf-idf进行加权，得到加权后的向量表示\n",
    "        self.tfidf_model = models.TfidfModel(corpus)\n",
    "        self.corpus_tfidf = self.tfidf_model[corpus]\n",
    "        \n",
    "        self.keyword_num = keyword_num\n",
    "        self.num_topics = num_topics\n",
    "        \n",
    "        # 选择加载的模型\n",
    "        if model == \"LSI\":\n",
    "            self.model = self.train_lsi()\n",
    "        else:\n",
    "            self.model = self.train_lda()\n",
    "            \n",
    "        # 得到数据集的主题-词分布\n",
    "        word_dic = self.word_dictionary(doc_list) \n",
    "        self.wordtopic_dic = self.get_wordtopic(word_dic)\n",
    "        \n",
    "    \n",
    "    # LSI的训练时根据现有的数据集生成文档-主题分布矩阵和主题-词分布矩阵，Gensim中有实现好的方法，可以直接调用。\n",
    "    def train_lsi(self):\n",
    "        lsi = models.LsiModel(self.corpus_tfidf, id2word=self.dictionary, num_topics=self.num_topics)\n",
    "        return lsi\n",
    "    \n",
    "    # LDA的训练时根据现有的数据集生成文档-主题分布矩阵和主题-词分布矩阵，Gensim中有实现好的方法，可以直接调用。\n",
    "    def train_lda(self):\n",
    "        lda = models.LdaModel(self.corpus_tfidf, id2word=self.dictionary, num_topics=self.num_topics)\n",
    "        return lda\n",
    "    \n",
    "    def get_wordtopic(self, word_dic):\n",
    "        wordtopic_dic = {}\n",
    "        for word in word_dic:\n",
    "            single_list = [word]\n",
    "            wordcorpus = self.tfidf_model[self.dictionary.doc2bow(single_list)]\n",
    "            wordtopic = self.model[wordcorpus]\n",
    "            wordtopic_dic[word] = wordtopic\n",
    "        return wordtopic_dic\n",
    "    \n",
    "    def get_simword(self, word_list):\n",
    "        # 计算词的分布和文档的分布的相似度，去相似度最高的keyword_num个词作为关键词\n",
    "        sentcorpus = self.tfidf_model[self.dictionary.doc2bow(word_list)]\n",
    "        senttopic = self.model[sentcorpus]\n",
    "        # 余弦相似度计算\n",
    "        def calsim(l1, l2):\n",
    "            a,b,c = 0.0, 0.0, 0.0\n",
    "            for t1, t2 in zip(l1, l2):\n",
    "                x1 = t1[1]\n",
    "                x2 = t2[1]\n",
    "                a += x1 * x1\n",
    "                b += x1 * x1\n",
    "                c += x2 * x2\n",
    "            sim = a / math.sqrt(b * c) if not (b * c) == 0.0 else 0.0\n",
    "            return sim\n",
    "        \n",
    "        # 计算输入文本和每个词的主题分布相似度\n",
    "        sim_dic = {}\n",
    "        for k, v in self.wordtopic_dic.items():\n",
    "            if k not in word_list:\n",
    "                continue\n",
    "            sim = calsim(v, senttopic)\n",
    "            sim_dic[k] = sim\n",
    "            \n",
    "        for k, v in sorted(sim_dic.items(), key=functools.cmp_to_key(cmp),reverse=True)[:self.keyword_num]:\n",
    "            print(k + '/' , end='')\n",
    "            \n",
    "        print()\n",
    "        \n",
    "    def word_dictionary(self, doc_list):\n",
    "        # 词空间构建方法和向量化方法，在没有gensim接口时的一般处理方法\n",
    "        dictionary = []\n",
    "        for doc in doc_list:\n",
    "            dictionary.extend(doc)\n",
    "        dictionary = list(set(dictionary))\n",
    "        return dictionary\n",
    "    \n",
    "    def doc2bowvec(self, word_list):\n",
    "        vec_list = [1 if word in word_list else 0 for word in self.dictionary]\n",
    "        return vec_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_extract(word_list, pos=False, keyword_num=10):\n",
    "    doc_list = load_data(pos)\n",
    "    idf_dic, default_idf = train_idf(doc_list)\n",
    "    tfidf_model = TfIdf(idf_dic, default_idf, word_list, keyword_num)\n",
    "    tfidf_model.get_tfidf()\n",
    "    \n",
    "def textrank_extract(text, pos=False, keyword_num=10):\n",
    "    textrank = analyse.textrank\n",
    "    keywords = textrank(text, keyword_num)\n",
    "    # 输出抽取出的关键词\n",
    "    for keyword in keywords:\n",
    "        print(keyword + \"/\", end='')\n",
    "    print()\n",
    "    \n",
    "def topic_extract(word_list, model, pos=False, keyword_num=10):\n",
    "    doc_list = load_data(pos)\n",
    "    topic_model = TopicModel(doc_list, keyword_num, model=model)\n",
    "    topic_model.get_simword(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "TF-IDF模型结果：\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-41-85d1e6a7190b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"TF-IDF模型结果：\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mtfidf_extract\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilter_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;31m# print(\"TextRank模型结果：\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m# textrank_extract(text)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-40-6fbfe7867e8a>\u001b[0m in \u001b[0;36mtfidf_extract\u001b[1;34m(word_list, pos, keyword_num)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mtfidf_extract\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeyword_num\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mdoc_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpos\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0midf_dic\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdefault_idf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_idf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mtfidf_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTfIdf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0midf_dic\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdefault_idf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeyword_num\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mtfidf_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_tfidf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-34-2a71ba9b74a0>\u001b[0m in \u001b[0;36mload_data\u001b[1;34m(pos, corpus_path)\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mcontent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# 每行的数据\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mseg_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mseg_to_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# 分词\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m         \u001b[0mfilter_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mword_filter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseg_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# 过滤停用词\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m         \u001b[0mdoc_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilter_list\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# 将处理后的结果保存到doc_list\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdoc_list\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-33-57f82a6d74ab>\u001b[0m in \u001b[0;36mword_filter\u001b[1;34m(seg_list, pos)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mword_filter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseg_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mstopword_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_stopword_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# 获取停用词表\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mfilter_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m# 保存过滤后的结果\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mseg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mseg_list\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-473df7028468>\u001b[0m in \u001b[0;36mget_stopword_list\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;31m# 进行编码转换确保匹配准确率\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mstop_word_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'./data/stopwords.txt'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mstopword_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0msw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\n'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstop_word_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mstopword_list\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.8_3.8.2544.0_x64__qbz5n2kfra8p0\\lib\\_bootlocale.py\u001b[0m in \u001b[0;36mgetpreferredencoding\u001b[1;34m(do_setlocale)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplatform\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"win\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[1;32mdef\u001b[0m \u001b[0mgetpreferredencoding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdo_setlocale\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutf8_mode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;34m'UTF-8'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "text = 'A fully differential calculation in perturbative quantum chromodynamics is presented for the production of massive photon pairs at hadron colliders. All next-to-leading order perturbative contributions from quark-antiquark, gluon-(anti)quark, and gluon-gluon subprocesses are included, as well as all-orders resummation of initial-state gluon radiation valid at next-to-next-to-leading logarithmic accuracy. The region of phase space is specified in which the calculation is most reliable. Good agreement is demonstrated with data from the Fermilab Tevatron, and predictions are made for more detailed tests with CDF and DO data. Predictions are shown for distributions of diphoton pairs produced at the energy of the Large Hadron Collider (LHC). Distributions of the diphoton pairs from the decay of a Higgs boson are contrasted with those produced from QCD processes at the LHC, showing that enhanced sensitivity to the signal can be obtained with judicious selection of events'\n",
    "\n",
    "pos = False\n",
    "seg_list = seg_to_list(text, pos)\n",
    "filter_list = word_filter(seg_list, pos)\n",
    "    \n",
    "print(\"TF-IDF模型结果：\")\n",
    "tfidf_extract(filter_list)\n",
    "# print(\"TextRank模型结果：\")\n",
    "# textrank_extract(text)\n",
    "# print(\"LSI模型结果：\")\n",
    "# topic_extract(filter_list, 'LSI', pos)\n",
    "print(\"LDA模型结果：\")\n",
    "topic_extract(filter_list, 'LDA', pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}